{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving MNIST with GZIP and code-golfing into obscurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and splitting MNIST data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakobserlier\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\datasets\\_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a training set...\n",
      "Creating a test set...\n"
     ]
    }
   ],
   "source": [
    "# Fetch and split MNIST data\n",
    "print(\"Fetching and splitting MNIST data...\")\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X, y = mnist.data.astype('float32'), mnist.target.astype('int64')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a training set...\n",
      "Creating a test set...\n"
     ]
    }
   ],
   "source": [
    "# Create a training set with 20 samples for each label\n",
    "print(\"Creating a training set...\")\n",
    "training_set = []\n",
    "for label in np.unique(y_train):\n",
    "    label_data = X_train[y_train == label].head(100)\n",
    "    samples = [(row.values, label) for _, row in label_data.iterrows()]\n",
    "    training_set.extend(samples)\n",
    "\n",
    "# Create a test set\n",
    "print(\"Creating a test set...\")\n",
    "test_set_df = X_test.copy()\n",
    "test_set_df['label'] = y_test\n",
    "test_set = [(row.drop('label').values, row['label']) for index, row in test_set_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:37<00:00,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.00%\n",
      "Generating confusion matrix...\n",
      "[[ 8  0  0  0  0  0  2  0  1  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  6  0  0  0  0  0  1  0]\n",
      " [ 1  0  0  8  0  0  0  0  2  0]\n",
      " [ 1  0  0  0  4  0  0  0  0  2]\n",
      " [ 1  0  0  0  0  6  0  0  2  0]\n",
      " [ 0  0  0  0  0  1 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  8  2  4]\n",
      " [ 0  0  0  1  0  0  0  0  9  1]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_ncd(x1, x2):\n",
    "    \"\"\"Compute the Normalized Compression Distance (NCD) between two samples.\"\"\"\n",
    "    Cx1 = len(gzip.compress(x1.tobytes()))\n",
    "    Cx2 = len(gzip.compress(x2.tobytes()))\n",
    "    Cx1x2 = len(gzip.compress((x1 + x2).tobytes()))\n",
    "    \n",
    "    return (Cx1x2 - min(Cx1, Cx2)) / max(Cx1, Cx2)\n",
    "\n",
    "print(\"Classifying test samples...\")\n",
    "\n",
    "k = 5  # Number of neighbors to consider\n",
    "correct_predictions = 0  # Counter for correct predictions\n",
    "actual_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Cache compressed lengths for training samples\n",
    "compressed_lengths = [(x, len(gzip.compress(x.tobytes())), label) for x, label in training_set]\n",
    "\n",
    "for (x1, actual_label) in tqdm(test_set[:100]):\n",
    "    # Calculate NCD for each training sample\n",
    "    distances = [(compute_ncd(x1, x), label) for x, _, label in compressed_lengths]\n",
    "    \n",
    "    # Get k nearest neighbors and predict label\n",
    "    neighbors = sorted(distances, key=lambda x: x[0])[:k]\n",
    "    top_k_class = [label for _, label in neighbors]\n",
    "    predicted_class = Counter(top_k_class).most_common(1)[0][0]\n",
    "    \n",
    "    # Update predictions and counts\n",
    "    actual_labels.append(actual_label)\n",
    "    predicted_labels.append(predicted_class)\n",
    "    correct_predictions += (predicted_class == actual_label)\n",
    "\n",
    "# Print results and confusion matrix\n",
    "print(f\"Accuracy: {correct_predictions / len(test_set[:100]) * 100:.2f}%\")\n",
    "print(\"Generating confusion matrix...\")\n",
    "print(confusion_matrix(actual_labels, predicted_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code golfed\n",
    "\n",
    "Code golfed into obscurity.\n",
    "\n",
    "For a more accurate accuracy, use more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = test_set[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = lambda z: len(gzip.compress(z.tobytes()))\n",
    "\n",
    "def ncd(x, y):\n",
    "    return (c(x + y) - min(c(x), c(y))) / max(c(x), c(y))\n",
    "\n",
    "cls = [(x, c(x), l) for x, l in training_set]\n",
    "\n",
    "correct_predictions = sum([np.array_equal(Counter([l for _, _, l in sorted([(ncd(x1, x), x, l) for x, _, l in cls], key=lambda t: t[0])[:5]]).most_common(1)[0][0], label) for x1, label in ts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", correct_predictions / len(ts) * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
